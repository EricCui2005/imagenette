{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Global Class and Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch as torch\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from typing import List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# Softmax image classifier class\n",
    "class SoftmaxClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(SoftmaxClassifier, self).__init__()\n",
    "        \n",
    "        # Single fully connected layer\n",
    "        self.linear = nn.Linear(input_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Compute raw logits\n",
    "        logits = self.linear(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# Function to get the input dimension of a colored image\n",
    "def get_input_dim(image_path):\n",
    "    img_array = cv2.imread(image_path)\n",
    "    flattened_array = img_array.reshape(-1)\n",
    "    return len(flattened_array)\n",
    "\n",
    "\n",
    "# Function to flatten a colored image into a 1D array\n",
    "def flatten_image(image_path):\n",
    "    img_array = cv2.imread(image_path)\n",
    "    img_array = cv2.resize(img_array, (426, 320))\n",
    "    flattened_array = torch.from_numpy(img_array.reshape(-1)).float()\n",
    "    flattened_array = flattened_array\n",
    "    return flattened_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Miscellaneous Testing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to resize an image to 426x320\n",
    "def resize_image(image_path):\n",
    "    # Read image\n",
    "    img = cv2.imread(image_path)\n",
    "    # Resize to 426x320\n",
    "    resized_img = cv2.resize(img, (426, 320))\n",
    "    return resized_img\n",
    "\n",
    "\n",
    "# Using OpenCV (cv2)\n",
    "def save_as_jpeg_cv2(numpy_array, output_path):\n",
    "    \"\"\"\n",
    "    Save a numpy array as a JPEG image.\n",
    "    \n",
    "    Args:\n",
    "        numpy_array: NumPy array of image (height, width, 3) in BGR format\n",
    "        output_path: String path where to save the image (e.g., 'output.jpg')\n",
    "    \"\"\"\n",
    "    success = cv2.imwrite(output_path, numpy_array)\n",
    "    if success:\n",
    "        print(f\"Image successfully saved to {output_path}\")\n",
    "    else:\n",
    "        print(\"Failed to save image\")\n",
    "        \n",
    "\n",
    "# Count total files in directory and subdirectories\n",
    "def count_files(directory):\n",
    "    total = 0\n",
    "\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        total += len(files)\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "init_image_path = \"/Users/ericcui/repos/imagenette/imagenette2-320/train/n01440764/ILSVRC2012_val_00000293.JPEG\"\n",
    "num_classes = 10\n",
    "num_epochs = 10\n",
    "\n",
    "# CSV processing configuration\n",
    "train_image_label_csv = \"/Users/ericcui/repos/imagenette/imagenette2-320/randomized_train.csv\"\n",
    "val_image_label_csv = \"/Users/ericcui/repos/imagenette/imagenette2-320/val_imagenette.csv\"\n",
    "path_prefix = \"/Users/ericcui/repos/imagenette/imagenette2-320/\"\n",
    "\n",
    "# Label to index mappings\n",
    "label_index_mappings = {\n",
    "    \"n01440764\": 0,\n",
    "    \"n02102040\": 1,\n",
    "    \"n02979186\": 2,\n",
    "    \"n03000684\": 3,\n",
    "    \"n03028079\": 4,\n",
    "    \"n03394916\": 5,\n",
    "    \"n03417042\": 6,\n",
    "    \"n03425413\": 7,\n",
    "    \"n03445777\": 8,\n",
    "    \"n03888257\": 9,\n",
    "}\n",
    "\n",
    "# Label to class mappings\n",
    "class_mappings = {\n",
    "    \"n01440764\": \"tench\",\n",
    "    \"n02102040\": \"English springer\",\n",
    "    \"n02979186\": \"cassette player\",\n",
    "    \"n03000684\": \"chain saw\",\n",
    "    \"n03028079\": \"church\",\n",
    "    \"n03394916\": \"French horn\",\n",
    "    \"n03417042\": \"garbage truck\",\n",
    "    \"n03425413\": \"gas pump\",\n",
    "    \"n03445777\": \"golf ball\",\n",
    "    \"n03888257\": \"parachute\"\n",
    "}\n",
    "\n",
    "def process_batch_images(batch_df: pd.DataFrame) -> List[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Process a batch of images from a DataFrame into a list of flattened image tensors.\n",
    "    \n",
    "    Args:\n",
    "        batch_df (pd.DataFrame): DataFrame containing image paths in a 'path' column\n",
    "        \n",
    "    Returns:\n",
    "        List[torch.Tensor]: List of flattened image tensors\n",
    "    \"\"\"\n",
    "    images = batch_df['path'].tolist()\n",
    "    images = [path_prefix + path for path in images]\n",
    "    images = [flatten_image(path) for path in images]\n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "base_learning_rate = 0.0001\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Initialization and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the softmax classifier using Metal Performance Shaders (MPS)\n",
    "input_dim = get_input_dim(init_image_path)\n",
    "softmax_classifier = SoftmaxClassifier(input_dim, num_classes)\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = torch.optim.SGD(softmax_classifier.parameters(), lr=base_learning_rate)\n",
    "\n",
    "# Read the CSV file containing image paths and labels\n",
    "df = pd.read_csv(train_image_label_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Validation Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_validation_images(val_df: pd.DataFrame) -> List[Tuple[torch.Tensor, int]]:\n",
    "    \"\"\"\n",
    "    Process validation images and labels from a DataFrame into a list of tuples.\n",
    "    \n",
    "    Args:\n",
    "        val_df (pd.DataFrame): DataFrame containing validation image paths and labels\n",
    "        \n",
    "    Returns:\n",
    "        List[Tuple[torch.Tensor, int]]: List of tuples containing (flattened_image_tensor, label_index)\n",
    "    \"\"\"\n",
    "    # Empty list to store validation tuples of (image, label) where image is a flattened tensor and label is an integer\n",
    "    val_tuples = []\n",
    "    \n",
    "    # Reading validation labels\n",
    "    val_labels = val_df['noisy_labels_0'].tolist()\n",
    "    val_labels = [label_index_mappings[label] for label in val_labels]\n",
    "\n",
    "    # Processing validation images\n",
    "    for i, row in tqdm(val_df.iterrows(), desc=\"Processing validation images\", total=len(val_df)):\n",
    "        val_tuple = (flatten_image(path_prefix + row['path']), val_labels[i])\n",
    "        val_tuples.append(val_tuple)\n",
    "    \n",
    "    return val_tuples\n",
    "\n",
    "\n",
    "def get_validation_accuracy(val_tuples: List[Tuple[torch.Tensor, int]]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate validation accuracy by comparing model predictions to true labels.\n",
    "    \n",
    "    Args:\n",
    "        val_tuples (List[Tuple[torch.Tensor, int]]): List of tuples containing (flattened_image_tensor, label_index)\n",
    "        \n",
    "    Returns:\n",
    "        float: Validation accuracy as a fraction between 0 and 1\n",
    "    \"\"\"\n",
    "    # Process validation data in batches for better efficiency\n",
    "    correct = 0\n",
    "    total = len(val_tuples)\n",
    "    batch_size = 32  # Process multiple images at once\n",
    "    \n",
    "    for i in range(0, total, batch_size):\n",
    "        batch = val_tuples[i:i+batch_size]\n",
    "        images = torch.stack([t[0] for t in batch])\n",
    "        labels = torch.tensor([t[1] for t in batch])\n",
    "        \n",
    "        with torch.no_grad():  # Disable gradient computation for inference\n",
    "            predictions = torch.argmax(softmax_classifier(images), dim=1)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            \n",
    "    return correct / total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading validation data and converting validation images to flattened tensors\n",
    "val_df = pd.read_csv(val_image_label_csv)[:200]\n",
    "val_tuples = process_validation_images(val_df)\n",
    "\n",
    "# Loss and accuracy tracking\n",
    "iteration_losses = []\n",
    "iteration_accuracies = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "    for i in tqdm(range(0, len(df), batch_size), desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        \n",
    "        # Get batch data\n",
    "        batch_df = df.iloc[i:i+batch_size]\n",
    "        \n",
    "        # Convert images and labels\n",
    "        images = process_batch_images(batch_df)\n",
    "        labels = batch_df['noisy_labels_0'].tolist()\n",
    "        labels = [label_index_mappings[label] for label in labels]\n",
    "        labels = torch.tensor(labels)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = softmax_classifier(torch.stack(images))\n",
    "        loss = nn.functional.cross_entropy(outputs, labels)\n",
    "\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Backpropagate to compute gradients and stepping to update weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculating validation accuracy for the model for the current iteration\n",
    "        iteration_accuracy = get_validation_accuracy(val_tuples)\n",
    "        \n",
    "        # Tracking iteration losses and accuracies\n",
    "        iteration_losses.append(loss.item())\n",
    "        iteration_accuracies.append(iteration_accuracy)\n",
    "        \n",
    "        # Updating epoch loss and iteration losses for tracking\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_accuracy += iteration_accuracy\n",
    "    \n",
    "    # Print average loss for the epoch\n",
    "    avg_epoch_loss = epoch_loss / (len(df) // batch_size)\n",
    "    avg_epoch_accuracy = epoch_accuracy / (len(df) // batch_size)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_epoch_loss:.4f}\")\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Accuracy: {avg_epoch_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure with two subplots\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "# Plot iteration losses\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(iteration_losses)\n",
    "plt.title('Training Loss per Iteration')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# Plot iteration accuracies \n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(iteration_accuracies)\n",
    "plt.title('Validation Accuracy per Iteration')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = get_validation_accuracy(val_tuples)\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
