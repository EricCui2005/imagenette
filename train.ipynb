{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Global Class and Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch as torch\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Softmax image classifier class\n",
    "class SoftmaxClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(SoftmaxClassifier, self).__init__()\n",
    "        \n",
    "        # Single fully connected layer\n",
    "        self.linear = nn.Linear(input_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Compute raw logits\n",
    "        logits = self.linear(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# Function to get the input dimension of a colored image\n",
    "def get_input_dim(image_path):\n",
    "    img_array = cv2.imread(image_path)\n",
    "    flattened_array = img_array.reshape(-1)\n",
    "    return len(flattened_array)\n",
    "\n",
    "\n",
    "# Function to flatten a colored image into a 1D array\n",
    "def flatten_image(image_path):\n",
    "    img_array = cv2.imread(image_path)\n",
    "    img_array = cv2.resize(img_array, (426, 320))\n",
    "    flattened_array = torch.from_numpy(img_array.reshape(-1)).float()\n",
    "    return flattened_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Miscellaneous Testing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to resize an image to 426x320\n",
    "def resize_image(image_path):\n",
    "    # Read image\n",
    "    img = cv2.imread(image_path)\n",
    "    # Resize to 426x320\n",
    "    resized_img = cv2.resize(img, (426, 320))\n",
    "    return resized_img\n",
    "\n",
    "\n",
    "# Using OpenCV (cv2)\n",
    "def save_as_jpeg_cv2(numpy_array, output_path):\n",
    "    \"\"\"\n",
    "    Save a numpy array as a JPEG image.\n",
    "    \n",
    "    Args:\n",
    "        numpy_array: NumPy array of image (height, width, 3) in BGR format\n",
    "        output_path: String path where to save the image (e.g., 'output.jpg')\n",
    "    \"\"\"\n",
    "    success = cv2.imwrite(output_path, numpy_array)\n",
    "    if success:\n",
    "        print(f\"Image successfully saved to {output_path}\")\n",
    "    else:\n",
    "        print(\"Failed to save image\")\n",
    "        \n",
    "\n",
    "# Count total files in directory and subdirectories\n",
    "def count_files(directory):\n",
    "    total = 0\n",
    "\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        total += len(files)\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "init_image_path = \"/Users/ericcui/repos/imagenette/imagenette2-320/train/n01440764/ILSVRC2012_val_00000293.JPEG\"\n",
    "num_classes = 10\n",
    "num_epochs = 10\n",
    "\n",
    "# CSV processing configuration\n",
    "train_image_label_csv = \"/Users/ericcui/repos/imagenette/imagenette2-320/train_imagenette.csv\"\n",
    "val_image_label_csv = \"/Users/ericcui/repos/imagenette/imagenette2-320/val_imagenette.csv\"\n",
    "path_prefix = \"/Users/ericcui/repos/imagenette/imagenette2-320/\"\n",
    "\n",
    "# Label to index mappings\n",
    "label_index_mappings = {\n",
    "    \"n01440764\": 0,\n",
    "    \"n02102040\": 1,\n",
    "    \"n02979186\": 2,\n",
    "    \"n03000684\": 3,\n",
    "    \"n03028079\": 4,\n",
    "    \"n03394916\": 5,\n",
    "    \"n03417042\": 6,\n",
    "    \"n03425413\": 7,\n",
    "    \"n03445777\": 8,\n",
    "    \"n03888257\": 9,\n",
    "}\n",
    "\n",
    "# Label to class mappings\n",
    "class_mappings = {\n",
    "    \"n01440764\": \"tench\",\n",
    "    \"n02102040\": \"English springer\",\n",
    "    \"n02979186\": \"cassette player\",\n",
    "    \"n03000684\": \"chain saw\",\n",
    "    \"n03028079\": \"church\",\n",
    "    \"n03394916\": \"French horn\",\n",
    "    \"n03417042\": \"garbage truck\",\n",
    "    \"n03425413\": \"gas pump\",\n",
    "    \"n03445777\": \"golf ball\",\n",
    "    \"n03888257\": \"parachute\"\n",
    "}\n",
    "\n",
    "# Function to convert images from image paths to flattened tensors in a batch dataframe\n",
    "def process_batch_images(batch_df):\n",
    "    images = batch_df['path'].tolist()\n",
    "    images = [path_prefix + path for path in images]\n",
    "    images = [flatten_image(path) for path in images]\n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "base_learning_rate = 0.001\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Initialization and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the softmax classifier\n",
    "input_dim = get_input_dim(init_image_path)\n",
    "softmax_classifier = SoftmaxClassifier(input_dim, num_classes)\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = torch.optim.Adam(softmax_classifier.parameters(), lr=base_learning_rate)\n",
    "\n",
    "# Read the CSV file containing image paths and labels\n",
    "df = pd.read_csv(train_image_label_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prototyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_validation_images(val_df: pd.DataFrame) -> List[Tuple[torch.Tensor, int]]:\n",
    "    \"\"\"\n",
    "    Process validation images and labels from a DataFrame into a list of tuples.\n",
    "    \n",
    "    Args:\n",
    "        val_df (pd.DataFrame): DataFrame containing validation image paths and labels\n",
    "        \n",
    "    Returns:\n",
    "        List[Tuple[torch.Tensor, int]]: List of tuples containing (flattened_image_tensor, label_index)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Empty list to store validation tuples of (image, label) where image is a flattened tensor and label is an integer\n",
    "    val_tuples = []\n",
    "    \n",
    "    # Reading validation labels\n",
    "    val_labels = val_df['noisy_labels_0'].tolist()\n",
    "    val_labels = [label_index_mappings[label] for label in val_labels]\n",
    "\n",
    "    # Processing validation images\n",
    "    for i, row in tqdm(val_df.iterrows(), desc=\"Processing validation images\", total=len(val_df)):\n",
    "        val_tuple = (flatten_image(path_prefix + row['path']), val_labels[i])\n",
    "        val_tuples.append(val_tuple)\n",
    "    \n",
    "    return val_tuples\n",
    "\n",
    "\n",
    "def get_validation_accuracy(val_tuples: List[Tuple[torch.Tensor, int]]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate validation accuracy by comparing model predictions to true labels.\n",
    "    \n",
    "    Args:\n",
    "        val_tuples (List[Tuple[torch.Tensor, int]]): List of tuples containing (flattened_image_tensor, label_index)\n",
    "        \n",
    "    Returns:\n",
    "        float: Validation accuracy as a fraction between 0 and 1\n",
    "    \"\"\"\n",
    "    \n",
    "    # Process validation data in batches for better efficiency\n",
    "    correct = 0\n",
    "    total = len(val_tuples)\n",
    "    batch_size = 32  # Process multiple images at once\n",
    "    \n",
    "    for i in range(0, total, batch_size):\n",
    "        batch = val_tuples[i:i+batch_size]\n",
    "        images = torch.stack([t[0] for t in batch])\n",
    "        labels = torch.tensor([t[1] for t in batch])\n",
    "        \n",
    "        with torch.no_grad():  # Disable gradient computation for inference\n",
    "            predictions = torch.argmax(softmax_classifier(images), dim=1)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            \n",
    "    return correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing validation images: 100%|██████████| 100/100 [00:00<00:00, 573.37it/s]\n"
     ]
    }
   ],
   "source": [
    "# Reading validation data and converting validation images to flattened tensors\n",
    "val_df = pd.read_csv(val_image_label_csv)[:100]\n",
    "val_tuples = process_validation_images(val_df)\n",
    "\n",
    "# Loss and accuracy tracking\n",
    "iteration_losses = []\n",
    "iteration_accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:  35%|███▌      | 105/296 [00:07<00:14, 13.45it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m optimizer.step()\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Calculating validation accuracy for the model for the current iteration\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m iteration_accuracy = \u001b[43mget_validation_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_tuples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Tracking iteration losses and accuracies\u001b[39;00m\n\u001b[32m     29\u001b[39m iteration_losses.append(loss.item())\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 49\u001b[39m, in \u001b[36mget_validation_accuracy\u001b[39m\u001b[34m(val_tuples)\u001b[39m\n\u001b[32m     46\u001b[39m     labels = torch.tensor([t[\u001b[32m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m batch])\n\u001b[32m     48\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():  \u001b[38;5;66;03m# Disable gradient computation for inference\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m         predictions = torch.argmax(\u001b[43msoftmax_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m, dim=\u001b[32m1\u001b[39m)\n\u001b[32m     50\u001b[39m         correct += (predictions == labels).sum().item()\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m correct / total\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/imagenette/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/imagenette/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mSoftmaxClassifier.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     18\u001b[39m \n\u001b[32m     19\u001b[39m     \u001b[38;5;66;03m# Compute raw logits\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/imagenette/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/imagenette/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/imagenette/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for i in tqdm(range(0, len(df), batch_size), desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        \n",
    "        # Get batch data\n",
    "        batch_df = df.iloc[i:i+batch_size]\n",
    "        \n",
    "        # Convert images and labels\n",
    "        images = process_batch_images(batch_df)\n",
    "        labels = batch_df['noisy_labels_0'].tolist()\n",
    "        labels = [label_index_mappings[label] for label in labels]\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = softmax_classifier(torch.stack(images))\n",
    "        loss = nn.functional.cross_entropy(outputs, torch.tensor(labels))\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Backpropagate to compute gradients and stepping to update weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculating validation accuracy for the model for the current iteration\n",
    "        iteration_accuracy = get_validation_accuracy(val_tuples)\n",
    "        \n",
    "        # Tracking iteration losses and accuracies\n",
    "        iteration_losses.append(loss.item())\n",
    "        iteration_accuracies.append(iteration_accuracy)\n",
    "        \n",
    "        # Updating epoch loss and iteration losses for tracking\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    # Print average loss for the epoch\n",
    "    avg_epoch_loss = epoch_loss / (len(df) // batch_size)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n",
      "105\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 549614.75, 509799.40625, 434558.0, 354462.78125, 264981.09375, 136213.25, 31550.060546875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 541127.0, 489899.15625, 442566.3125, 383434.40625, 267744.625, 170549.28125, 68584.265625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 882658.0, 969011.8125, 938692.5, 836117.375, 740284.375, 611244.875, 432203.4375, 278863.40625, 131934.828125, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "print(len(iteration_losses))\n",
    "print(len(iteration_accuracies))\n",
    "\n",
    "print(iteration_losses)\n",
    "print(iteration_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
