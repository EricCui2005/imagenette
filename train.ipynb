{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch as torch\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Class Definition and Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax image classifier class\n",
    "class SoftmaxClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(SoftmaxClassifier, self).__init__()\n",
    "        \n",
    "        # Single fully connected layer\n",
    "        self.linear = nn.Linear(input_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Compute raw logits\n",
    "        logits = self.linear(x)\n",
    "        \n",
    "        # Apply softmax to get probabilities\n",
    "        probabilities = torch.softmax(logits, dim=1)\n",
    "        return probabilities\n",
    "\n",
    "# Function to get the input dimension of a colored image\n",
    "def get_input_dim(image_path):\n",
    "    img_array = cv2.imread(image_path)\n",
    "    flattened_array = img_array.reshape(-1)\n",
    "    return len(flattened_array)\n",
    "\n",
    "# Function to flatten a colored image into a 1D array\n",
    "def flatten_image(image_path):\n",
    "    img_array = cv2.imread(image_path)\n",
    "    img_array = cv2.resize(img_array, (426, 320))\n",
    "    flattened_array = img_array.reshape(-1)\n",
    "    return flattened_array\n",
    "\n",
    "# Count total files in directory and subdirectories\n",
    "def count_files(directory):\n",
    "    total = 0\n",
    "\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        total += len(files)\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "init_image_path = \"/Users/ericcui/repos/imagenette/imagenette2-320/train/n01440764/ILSVRC2012_val_00000293.JPEG\"\n",
    "num_classes = 10\n",
    "num_epochs = 10\n",
    "base_learning_rate = 0.001\n",
    "batch_size = 32\n",
    "\n",
    "# CSV processing configuration\n",
    "train_image_label_csv = \"/Users/ericcui/repos/imagenette/imagenette2-320/train_imagenette.csv\"\n",
    "path_prefix = \"/Users/ericcui/repos/imagenette/imagenette2-320/\"\n",
    "\n",
    "# Label to index mappings\n",
    "label_index_mappings = {\n",
    "    \"n01440764\": 0,\n",
    "    \"n02102040\": 1,\n",
    "    \"n02979186\": 2,\n",
    "    \"n03000684\": 3,\n",
    "    \"n03028079\": 4,\n",
    "    \"n03394916\": 5,\n",
    "    \"n03417042\": 6,\n",
    "    \"n03425413\": 7,\n",
    "    \"n03445777\": 8,\n",
    "    \"n03888257\": 9,\n",
    "}\n",
    "\n",
    "# Label to class mappings\n",
    "class_mappings = {\n",
    "    \"n01440764\": \"tench\",\n",
    "    \"n02102040\": \"English springer\",\n",
    "    \"n02979186\": \"cassette player\",\n",
    "    \"n03000684\": \"chain saw\",\n",
    "    \"n03028040\": \"church\",\n",
    "    \"n03394916\": \"French horn\",\n",
    "    \"n03417042\": \"guitar\",\n",
    "    \"n03425413\": \"hammer\",\n",
    "    \"n03445722\": \"harmonica\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Miscellaneous Testing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to resize an image to 426x320\n",
    "def resize_image(image_path):\n",
    "    # Read image\n",
    "    img = cv2.imread(image_path)\n",
    "    # Resize to 426x320\n",
    "    resized_img = cv2.resize(img, (426, 320))\n",
    "    return resized_img\n",
    "\n",
    "\n",
    "# Using OpenCV (cv2)\n",
    "def save_as_jpeg_cv2(numpy_array, output_path):\n",
    "    \"\"\"\n",
    "    Save a numpy array as a JPEG image.\n",
    "    \n",
    "    Args:\n",
    "        numpy_array: NumPy array of image (height, width, 3) in BGR format\n",
    "        output_path: String path where to save the image (e.g., 'output.jpg')\n",
    "    \"\"\"\n",
    "    success = cv2.imwrite(output_path, numpy_array)\n",
    "    if success:\n",
    "        print(f\"Image successfully saved to {output_path}\")\n",
    "    else:\n",
    "        print(\"Failed to save image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prototyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the softmax classifier\n",
    "input_dim = get_input_dim(init_image_path)\n",
    "softmax_classifier = SoftmaxClassifier(input_dim, num_classes)\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = torch.optim.Adam(softmax_classifier.parameters(), lr=base_learning_rate)\n",
    "\n",
    "# Read the CSV file containing image paths and labels\n",
    "df = pd.read_csv(train_image_label_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>noisy_labels_0</th>\n",
       "      <th>noisy_labels_1</th>\n",
       "      <th>noisy_labels_5</th>\n",
       "      <th>noisy_labels_25</th>\n",
       "      <th>noisy_labels_50</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train/n02979186/n02979186_9036.JPEG</td>\n",
       "      <td>n02979186</td>\n",
       "      <td>n02979186</td>\n",
       "      <td>n02979186</td>\n",
       "      <td>n02979186</td>\n",
       "      <td>n02979186</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train/n02979186/n02979186_11957.JPEG</td>\n",
       "      <td>n02979186</td>\n",
       "      <td>n02979186</td>\n",
       "      <td>n02979186</td>\n",
       "      <td>n02979186</td>\n",
       "      <td>n03000684</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train/n02979186/n02979186_9715.JPEG</td>\n",
       "      <td>n02979186</td>\n",
       "      <td>n02979186</td>\n",
       "      <td>n02979186</td>\n",
       "      <td>n03417042</td>\n",
       "      <td>n03000684</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train/n02979186/n02979186_21736.JPEG</td>\n",
       "      <td>n02979186</td>\n",
       "      <td>n02979186</td>\n",
       "      <td>n02979186</td>\n",
       "      <td>n02979186</td>\n",
       "      <td>n03417042</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train/n02979186/ILSVRC2012_val_00046953.JPEG</td>\n",
       "      <td>n02979186</td>\n",
       "      <td>n02979186</td>\n",
       "      <td>n02979186</td>\n",
       "      <td>n02979186</td>\n",
       "      <td>n03394916</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           path noisy_labels_0 noisy_labels_1  \\\n",
       "0           train/n02979186/n02979186_9036.JPEG      n02979186      n02979186   \n",
       "1          train/n02979186/n02979186_11957.JPEG      n02979186      n02979186   \n",
       "2           train/n02979186/n02979186_9715.JPEG      n02979186      n02979186   \n",
       "3          train/n02979186/n02979186_21736.JPEG      n02979186      n02979186   \n",
       "4  train/n02979186/ILSVRC2012_val_00046953.JPEG      n02979186      n02979186   \n",
       "\n",
       "  noisy_labels_5 noisy_labels_25 noisy_labels_50  is_valid  \n",
       "0      n02979186       n02979186       n02979186     False  \n",
       "1      n02979186       n02979186       n03000684     False  \n",
       "2      n02979186       n03417042       n03000684     False  \n",
       "3      n02979186       n02979186       n03417042     False  \n",
       "4      n02979186       n02979186       n03394916     False  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert images from image paths to flattened tensors in a batch dataframe\n",
    "def process_batch_images(batch_df):\n",
    "    images = batch_df['path'].tolist()\n",
    "    images = [path_prefix + path for path in images]\n",
    "    images = [flatten_image(path) for path in images]\n",
    "    \n",
    "    images = [torch.tensor(img, dtype=torch.float32) for img in images]\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m      6\u001b[39m outputs = softmax_classifier(torch.stack(images))\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m loss = nn.functional.cross_entropy(outputs, \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# # Backward pass and optimization\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# optimizer.zero_grad()\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# loss.backward()\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# optimizer.step()\u001b[39;00m\n\u001b[32m     15\u001b[39m \n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# print(f\"Loss: {loss.item():.4f}\")\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: too many dimensions 'str'"
     ]
    }
   ],
   "source": [
    "batch_df = df.iloc[0:32]\n",
    "images = process_batch_images(batch_df)\n",
    "labels = batch_df['noisy_labels_0'].tolist()[:32]\n",
    "\n",
    "# Forward pass\n",
    "outputs = softmax_classifier(torch.stack(images))\n",
    "\n",
    "\n",
    "loss = nn.functional.cross_entropy(outputs, torch.tensor(labels))\n",
    "\n",
    "# # Backward pass and optimization\n",
    "# optimizer.zero_grad()\n",
    "# loss.backward()\n",
    "# optimizer.step()\n",
    "\n",
    "# print(f\"Loss: {loss.item():.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "print(type(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[125]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m batch_df = df.iloc[i:i+batch_size]\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Converting images to flattened tensors and labels to list\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m images = \u001b[43mprocess_batch_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m labels = batch_df[\u001b[33m'\u001b[39m\u001b[33mnoisy_labels_0\u001b[39m\u001b[33m'\u001b[39m].tolist()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[124]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mprocess_batch_images\u001b[39m\u001b[34m(batch_df)\u001b[39m\n\u001b[32m      2\u001b[39m images = batch_df[\u001b[33m'\u001b[39m\u001b[33mpath\u001b[39m\u001b[33m'\u001b[39m].tolist()\n\u001b[32m      3\u001b[39m images = [path_prefix + path \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m images = [\u001b[43mflatten_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[32m      6\u001b[39m images = [torch.tensor(img, dtype=torch.float32) \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m images\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[97]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mflatten_image\u001b[39m\u001b[34m(image_path)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mflatten_image\u001b[39m(image_path):\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     img_array = \u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m     flattened_array = img_array.reshape(-\u001b[32m1\u001b[39m)\n\u001b[32m     28\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m flattened_array\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        \n",
    "        # Getting the dataframe for the batch\n",
    "        batch_df = df.iloc[i:i+batch_size]\n",
    "        \n",
    "        # Converting images to flattened tensors and labels to list\n",
    "        images = process_batch_images(batch_df)\n",
    "        labels = batch_df['noisy_labels_0'].tolist()\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
