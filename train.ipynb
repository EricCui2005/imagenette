{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch as torch\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Class Definition and Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax image classifier class\n",
    "class SoftmaxClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(SoftmaxClassifier, self).__init__()\n",
    "        \n",
    "        # Single fully connected layer\n",
    "        self.linear = nn.Linear(input_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Compute raw logits\n",
    "        logits = self.linear(x)\n",
    "        return logits\n",
    "\n",
    "# Function to get the input dimension of a colored image\n",
    "def get_input_dim(image_path):\n",
    "    img_array = cv2.imread(image_path)\n",
    "    flattened_array = img_array.reshape(-1)\n",
    "    return len(flattened_array)\n",
    "\n",
    "# Function to flatten a colored image into a 1D array\n",
    "def flatten_image(image_path):\n",
    "    img_array = cv2.imread(image_path)\n",
    "    img_array = cv2.resize(img_array, (426, 320))\n",
    "    flattened_array = img_array.reshape(-1)\n",
    "    return flattened_array\n",
    "\n",
    "# Count total files in directory and subdirectories\n",
    "def count_files(directory):\n",
    "    total = 0\n",
    "\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        total += len(files)\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Miscellaneous Testing Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "init_image_path = \"/Users/ericcui/repos/imagenette/imagenette2-320/train/n01440764/ILSVRC2012_val_00000293.JPEG\"\n",
    "num_classes = 10\n",
    "num_epochs = 10\n",
    "\n",
    "# CSV processing configuration\n",
    "train_image_label_csv = \"/Users/ericcui/repos/imagenette/imagenette2-320/train_imagenette.csv\"\n",
    "path_prefix = \"/Users/ericcui/repos/imagenette/imagenette2-320/\"\n",
    "\n",
    "# Label to index mappings\n",
    "label_index_mappings = {\n",
    "    \"n01440764\": 0,\n",
    "    \"n02102040\": 1,\n",
    "    \"n02979186\": 2,\n",
    "    \"n03000684\": 3,\n",
    "    \"n03028079\": 4,\n",
    "    \"n03394916\": 5,\n",
    "    \"n03417042\": 6,\n",
    "    \"n03425413\": 7,\n",
    "    \"n03445777\": 8,\n",
    "    \"n03888257\": 9,\n",
    "}\n",
    "\n",
    "# Label to class mappings\n",
    "class_mappings = {\n",
    "    \"n01440764\": \"tench\",\n",
    "    \"n02102040\": \"English springer\",\n",
    "    \"n02979186\": \"cassette player\",\n",
    "    \"n03000684\": \"chain saw\",\n",
    "    \"n03028079\": \"church\",\n",
    "    \"n03394916\": \"French horn\",\n",
    "    \"n03417042\": \"garbage truck\",\n",
    "    \"n03425413\": \"gas pump\",\n",
    "    \"n03445777\": \"golf ball\",\n",
    "    \"n03888257\": \"parachute\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to resize an image to 426x320\n",
    "def resize_image(image_path):\n",
    "    # Read image\n",
    "    img = cv2.imread(image_path)\n",
    "    # Resize to 426x320\n",
    "    resized_img = cv2.resize(img, (426, 320))\n",
    "    return resized_img\n",
    "\n",
    "\n",
    "# Using OpenCV (cv2)\n",
    "def save_as_jpeg_cv2(numpy_array, output_path):\n",
    "    \"\"\"\n",
    "    Save a numpy array as a JPEG image.\n",
    "    \n",
    "    Args:\n",
    "        numpy_array: NumPy array of image (height, width, 3) in BGR format\n",
    "        output_path: String path where to save the image (e.g., 'output.jpg')\n",
    "    \"\"\"\n",
    "    success = cv2.imwrite(output_path, numpy_array)\n",
    "    if success:\n",
    "        print(f\"Image successfully saved to {output_path}\")\n",
    "    else:\n",
    "        print(\"Failed to save image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "base_learning_rate = 0.001\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prototyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the softmax classifier\n",
    "input_dim = get_input_dim(init_image_path)\n",
    "softmax_classifier = SoftmaxClassifier(input_dim, num_classes)\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = torch.optim.Adam(softmax_classifier.parameters(), lr=base_learning_rate)\n",
    "\n",
    "# Read the CSV file containing image paths and labels\n",
    "df = pd.read_csv(train_image_label_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert images from image paths to flattened tensors in a batch dataframe\n",
    "def process_batch_images(batch_df):\n",
    "    images = batch_df['path'].tolist()\n",
    "    images = [path_prefix + path for path in images]\n",
    "    images = [flatten_image(path) for path in images]\n",
    "    \n",
    "    images = [torch.tensor(img, dtype=torch.float32) for img in images]\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 135.4510\n"
     ]
    }
   ],
   "source": [
    "batch_df = df.iloc[0:32]\n",
    "images = process_batch_images(batch_df)\n",
    "labels = batch_df['noisy_labels_0'].tolist()[:32]\n",
    "# Convert string labels to indices using labels_index_mappings\n",
    "labels = [label_index_mappings[label] for label in labels]\n",
    "\n",
    "\n",
    "# Forward pass\n",
    "outputs = softmax_classifier(torch.stack(images))\n",
    "\n",
    "\n",
    "loss = nn.functional.cross_entropy(outputs, torch.tensor(labels))\n",
    "\n",
    "# Backward pass and optimization\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(f\"Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(0, len(df), batch_size), desc=\"Training Progress\"):\n",
    "        # Get batch data\n",
    "        batch_df = df.iloc[i:i+batch_size]\n",
    "        \n",
    "        # Convert images and labels\n",
    "        images = process_batch_images(batch_df)\n",
    "        labels = batch_df['noisy_labels_0'].tolist()[i:i+batch_size]\n",
    "        labels = [label_index_mappings[label] for label in labels]\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = softmax_classifier(torch.stack(images))\n",
    "        loss = nn.functional.cross_entropy(outputs, torch.tensor(labels))\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "# Track losses for plotting\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<NllLossBackward0 object at 0x166776590>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[125]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m batch_df = df.iloc[i:i+batch_size]\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Converting images to flattened tensors and labels to list\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m images = \u001b[43mprocess_batch_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m labels = batch_df[\u001b[33m'\u001b[39m\u001b[33mnoisy_labels_0\u001b[39m\u001b[33m'\u001b[39m].tolist()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[124]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mprocess_batch_images\u001b[39m\u001b[34m(batch_df)\u001b[39m\n\u001b[32m      2\u001b[39m images = batch_df[\u001b[33m'\u001b[39m\u001b[33mpath\u001b[39m\u001b[33m'\u001b[39m].tolist()\n\u001b[32m      3\u001b[39m images = [path_prefix + path \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m images = [\u001b[43mflatten_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[32m      6\u001b[39m images = [torch.tensor(img, dtype=torch.float32) \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m images\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[97]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mflatten_image\u001b[39m\u001b[34m(image_path)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mflatten_image\u001b[39m(image_path):\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     img_array = \u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m     flattened_array = img_array.reshape(-\u001b[32m1\u001b[39m)\n\u001b[32m     28\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m flattened_array\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        \n",
    "        # Getting the dataframe for the batch\n",
    "        batch_df = df.iloc[i:i+batch_size]\n",
    "        \n",
    "        # Converting images to flattened tensors and labels to list\n",
    "        images = process_batch_images(batch_df)\n",
    "        labels = batch_df['noisy_labels_0'].tolist()\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
